{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ProcessPool-formate data - provided data from jainils script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir_structure(base_dir, categories, subfolders):\n",
    "    for category in categories:\n",
    "        for subfolder in subfolders:\n",
    "            path = os.path.join(base_dir, category, subfolder, 'images')\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            path = os.path.join(base_dir, category, subfolder, 'labels')\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def copy_files(src_image_path, dest_image_path, src_label_path, dest_label_path):\n",
    "    shutil.copy(src_image_path, dest_image_path)\n",
    "    shutil.copy(src_label_path, dest_label_path)\n",
    "\n",
    "def split_data(src_dir, dest_dir, categories, split_ratio):\n",
    "    with ProcessPoolExecutor(max_workers=50) as executor:  # Adjust max_workers as per your CPU cores\n",
    "        for category in categories:\n",
    "            images = [f for f in os.listdir(os.path.join(src_dir, category, 'images')) if os.path.isfile(os.path.join(src_dir, category, 'images', f))]\n",
    "            labels = [f for f in os.listdir(os.path.join(src_dir, category, 'labels')) if os.path.isfile(os.path.join(src_dir, category, 'labels', f))]\n",
    "\n",
    "            combined = list(zip(images, labels))\n",
    "            random.shuffle(combined)\n",
    "            images[:], labels[:] = zip(*combined)\n",
    "\n",
    "            num_images = len(images)\n",
    "            train_split = int(split_ratio[0] * num_images)\n",
    "            test_split = int(split_ratio[1] * num_images) + train_split\n",
    "\n",
    "            datasets = {\n",
    "                'train': (images[:train_split], labels[:train_split]),\n",
    "                'test': (images[train_split:test_split], labels[train_split:test_split]),\n",
    "                'val': (images[test_split:], labels[test_split:])\n",
    "            }\n",
    "\n",
    "            for dataset in datasets:\n",
    "                for image, label in zip(datasets[dataset][0], datasets[dataset][1]):\n",
    "                    src_image_path = os.path.join(src_dir, category, 'images', image)\n",
    "                    dest_image_path = os.path.join(dest_dir, category, dataset, 'images', image)\n",
    "\n",
    "                    src_label_path = os.path.join(src_dir, category, 'labels', label)\n",
    "                    dest_label_path = os.path.join(dest_dir, category, dataset, 'labels', label)\n",
    "\n",
    "                    executor.submit(copy_files, src_image_path, dest_image_path, src_label_path, dest_label_path)\n",
    "\n",
    "def main():\n",
    "    src_dir = '/mnt/storage/backup/label_wise_jainil/'\n",
    "    dest_dir = '/home/research/kilasar_sentinal_wepon_detection/data'\n",
    "    categories = ['0', '1', '2', 'n']  # Specifying the 4 categories\n",
    "    subfolders = ['train', 'test', 'val']\n",
    "    split_ratio = (0.8, 0.1, 0.1)  # 80% train, 10% test, 10% validation\n",
    "\n",
    "    create_dir_structure(dest_dir, categories, subfolders)\n",
    "    split_data(src_dir, dest_dir, categories, split_ratio)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create subset from a large dataset in yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def copy_file(pair):\n",
    "    src, dest = pair\n",
    "    shutil.copy(src, dest)\n",
    "\n",
    "def create_subset(images_dir, labels_dir, subset_size, num_processes, subset_location):\n",
    "    # Create a list of image files and corresponding label files\n",
    "    image_files = sorted(os.listdir(images_dir))\n",
    "    label_files = sorted(os.listdir(labels_dir))\n",
    "\n",
    "    # Select the first subset_size files\n",
    "    image_files_subset = image_files[:subset_size]\n",
    "    label_files_subset = label_files[:subset_size]\n",
    "\n",
    "    # Create subset directory if it doesn't exist\n",
    "    subset_images_dir = os.path.join(subset_location, 'images')\n",
    "    subset_labels_dir = os.path.join(subset_location, 'labels')\n",
    "    os.makedirs(subset_images_dir, exist_ok=True)\n",
    "    os.makedirs(subset_labels_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare source and destination paths for copying\n",
    "    image_pairs = [(os.path.join(images_dir, img), os.path.join(subset_images_dir, img)) for img in image_files_subset]\n",
    "    label_pairs = [(os.path.join(labels_dir, lbl), os.path.join(subset_labels_dir, lbl)) for lbl in label_files_subset]\n",
    "\n",
    "    # Use multiprocessing Pool with specified number of processes\n",
    "    with Pool(processes=num_processes) as pool:\n",
    "        pool.map(copy_file, image_pairs)\n",
    "        pool.map(copy_file, label_pairs)\n",
    "\n",
    "\n",
    "# Define paths and parameters\n",
    "images_directory = \"/home/research/kilasar_sentinal_wepon_detection/data/n/val/images\"\n",
    "labels_directory = \"/home/research/kilasar_sentinal_wepon_detection/data/n/val/labels\"\n",
    "subset_size = 1200\n",
    "num_processes = 60  # Adjust this number based on your system's capabilities\n",
    "subset_location = \"/home/research/kilasar_sentinal_wepon_detection/suryansh/data/n/val\"  # Specify the location for the subset\n",
    "\n",
    "# Create the subset\n",
    "create_subset(images_directory, labels_directory, subset_size, num_processes, subset_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "def calculate_accuracy(model, data_dir, split='train'):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for class_idx in range(3):  # Adjust based on your actual class directories\n",
    "        image_folder = os.path.join(data_dir, str(class_idx), split, 'images')\n",
    "        label_folder = os.path.join(data_dir, str(class_idx), split, 'labels')\n",
    "        \n",
    "        image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "        for img_name in tqdm(image_files, desc=f\"Processing class {class_idx} ({split})\", unit=\"image\"):\n",
    "            img_path = os.path.join(image_folder, img_name)\n",
    "            label_path = os.path.join(label_folder, img_name.replace('.jpg', '.txt'))\n",
    "            \n",
    "            if not os.path.exists(label_path):\n",
    "                continue\n",
    "            temp_true = 0\n",
    "            # Load ground truth\n",
    "            with open(label_path, 'r') as f:\n",
    "                true_classes = [int(line.split()[0]) for line in f]\n",
    "                temp_true = true_classes[0]\n",
    "            \n",
    "            # Get predictions\n",
    "            results = model.predict(img_path, imgsz=640, device=0, verbose=False)  # specify batch size and device, disable verbose\n",
    "            pred_classes = [int(box.cls) for box in results[0].boxes]\n",
    "            if len(pred_classes) == 0:\n",
    "                y_true.extend([temp_true])\n",
    "                y_pred.extend([-1])\n",
    "            for pred in pred_classes:\n",
    "                if temp_true == pred:\n",
    "                    y_true.extend([temp_true])\n",
    "                    y_pred.extend([temp_true])\n",
    "                else:\n",
    "                    y_true.extend([temp_true])\n",
    "                    y_pred.extend([-1])\n",
    "            y_pred.extend(pred_classes)\n",
    "\n",
    "    # Handle case where there are no predictions or no true labels\n",
    "    if not y_pred:\n",
    "        y_pred = [0] * len(y_true)\n",
    "    if not y_true:\n",
    "        y_true = [0] * len(y_pred)\n",
    "    \n",
    "    # Ensure y_true and y_pred have the same length\n",
    "    min_length = min(len(y_true), len(y_pred))\n",
    "    y_true = y_true[:min_length]\n",
    "    y_pred = y_pred[:min_length]\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "model = YOLO(\"/home/research/yolo_test/runs/detect/train4/weights/last.pt\")\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy, train_precision, train_recall, train_f1 = calculate_accuracy(\n",
    "    model,\n",
    "    '/home/research/kilasar_sentinal_wepon_detection/data',\n",
    "    split='train'\n",
    ")\n",
    "print(f\"Training Accuracy: {train_accuracy}\")\n",
    "print(f\"Training Precision: {train_precision}\")\n",
    "print(f\"Training Recall: {train_recall}\")\n",
    "print(f\"Training F1 Score: {train_f1}\")\n",
    "\n",
    "# Calculate validation accuracy\n",
    "validation_accuracy, validation_precision, validation_recall, validation_f1 = calculate_accuracy(\n",
    "    model,\n",
    "    '/home/research/kilasar_sentinal_wepon_detection/data',\n",
    "    split='val'\n",
    ")\n",
    "print(f\"Validation Accuracy: {validation_accuracy}\")\n",
    "print(f\"Validation Precision: {validation_precision}\")\n",
    "print(f\"Validation Recall: {validation_recall}\")\n",
    "print(f\"Validation F1 Score: {validation_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.1884\n"
     ]
    }
   ],
   "source": [
    "def calculate_f1_score(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Example values\n",
    "precision = 0.105\n",
    "recall = 0.915\n",
    "\n",
    "f1_score = calculate_f1_score(precision, recall)\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert xml to txt yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "def get_image_size(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        return img.size  # returns (width, height)\n",
    "\n",
    "def resize_image(image_path, target_size):\n",
    "    with Image.open(image_path) as img:\n",
    "        resized_img = img.resize(target_size, Image.LANCZOS)\n",
    "        resized_img.save(image_path)\n",
    "\n",
    "def convert_xml_to_yolo(xml_file, classes, output_dir, image_size, original_image_size):\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        image_width = original_image_size[0]\n",
    "        image_height = original_image_size[1]\n",
    "        \n",
    "        yolo_annotations = []\n",
    "        \n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            if class_name not in classes:\n",
    "                continue\n",
    "            \n",
    "            class_id = classes.index(class_name)\n",
    "            bndbox = obj.find('bndbox')\n",
    "            xmin = float(bndbox.find('xmin').text)\n",
    "            ymin = float(bndbox.find('ymin').text)\n",
    "            xmax = float(bndbox.find('xmax').text)\n",
    "            ymax = float(bndbox.find('ymax').text)\n",
    "            \n",
    "            # Normalize coordinates for the original image size\n",
    "            x_center = (xmin + xmax) / 2 / image_width\n",
    "            y_center = (ymin + ymax) / 2 / image_height\n",
    "            width = (xmax - xmin) / image_width\n",
    "            height = (ymax - ymin) / image_height\n",
    "            \n",
    "            # Scale coordinates to the new image size\n",
    "            x_center *= image_size[0] / image_width\n",
    "            y_center *= image_size[1] / image_height\n",
    "            width *= image_size[0] / image_width\n",
    "            height *= image_size[1] / image_height\n",
    "            \n",
    "            yolo_annotations.append(f\"{class_id} {x_center} {y_center} {width} {height}\")\n",
    "        \n",
    "        # Write to YOLO annotation file with the same name as the XML file but with a .txt extension\n",
    "        output_file = os.path.join(output_dir, os.path.splitext(os.path.basename(xml_file))[0] + '.txt')\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write(\"\\n\".join(yolo_annotations))\n",
    "        \n",
    "        # print(f\"Processed: {xml_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {xml_file}: {e}\")\n",
    "\n",
    "def process_file(args):\n",
    "    xml_file, image_dir, classes, output_dir, image_size = args\n",
    "    try:\n",
    "        image_file = os.path.splitext(os.path.basename(xml_file))[0] + '.jpg'\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        \n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Image file missing: {image_path}\")\n",
    "            return\n",
    "        \n",
    "        original_image_size = get_image_size(image_path)\n",
    "        resize_image(image_path, image_size)\n",
    "        convert_xml_to_yolo(xml_file, classes, output_dir, image_size, original_image_size)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {xml_file}: {e}\")\n",
    "\n",
    "def convert_dataset(xml_dir, image_dir, classes, output_dir, image_size):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    xml_files = [os.path.join(xml_dir, f) for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=50) as executor:  # Use 32 cores\n",
    "        futures = [executor.submit(process_file, (xml_file, image_dir, classes, output_dir, image_size)) for xml_file in xml_files]\n",
    "        for future in as_completed(futures):\n",
    "            pass  # This will ensure we wait for all tasks to complete\n",
    "\n",
    "# Define the classes and target image size\n",
    "classes = [\"gun\"]\n",
    "target_image_size = (640, 480)  # Target image size\n",
    "\n",
    "# Directories\n",
    "xml_dir = \"/mnt/storage/kilsar_jainil/Train/Annotations/\"\n",
    "image_dir = \"/mnt/storage/kilsar_jainil/Train/JPEGImages/\"\n",
    "output_dir = \"/mnt/storage/kilsar_jainil/Train/Annotations_yolo4/\"\n",
    "\n",
    "convert_dataset(xml_dir, image_dir, classes, output_dir, target_image_size)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
