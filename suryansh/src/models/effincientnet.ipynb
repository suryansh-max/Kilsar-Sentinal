{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effecient Nets\n",
    "use EfficientNet models for real-time applications using TensorFlow Lite. TensorFlow Lite is designed to optimize models for deployment on edge devices, providing smaller model sizes and faster inference times. EfficientNet models, especially the smaller variants like B0-B3, are well-suited for deployment using TensorFlow Lite on devices with limited computational resources, such as smartphones, Raspberry Pi, or IoT devices.\n",
    "\n",
    "Steps to Use EfficientNet with TensorFlow Lite\n",
    "Convert EfficientNet Model to TensorFlow Lite Format:\n",
    "\n",
    "You need to convert the TensorFlow/Keras EfficientNet model to a .tflite format.\n",
    "Use TensorFlow’s tf.lite.TFLiteConverter to convert the trained model.\n",
    "Quantization:\n",
    "\n",
    "Apply quantization techniques (e.g., post-training quantization) to further reduce the model size and improve inference speed.\n",
    "Supported quantization types include float16, int8, and dynamic range.\n",
    "Deploy on Edge Device:\n",
    "\n",
    "Deploy the .tflite model on your target device, such as a Raspberry Pi or Android smartphone.\n",
    "Use TensorFlow Lite’s interpreter to load and run inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  \n",
    "import tflite \n",
    "print(tensorflow.__version__ , tflite.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to generate data generators\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch, lr):\n",
    "    if epoch > 5:\n",
    "        lr = lr * 0.1  # Reduce learning rate after 5 epochs\n",
    "    return lr\n",
    "\n",
    "lr_callback = LearningRateScheduler(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 14:18:25.167358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22400 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:4a:00.0, compute capability: 8.0\n",
      "2024-10-23 14:18:25.169442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22400 MB memory:  -> device: 1, name: NVIDIA A30, pci bus id: 0000:61:00.0, compute capability: 8.0\n",
      "2024-10-23 14:18:25.171372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22400 MB memory:  -> device: 2, name: NVIDIA A30, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2024-10-23 14:18:25.173247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22400 MB memory:  -> device: 3, name: NVIDIA A30, pci bus id: 0000:e1:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,573,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │    \u001b[38;5;34m10,783,535\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m1,573,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,046,705</span> (49.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,046,705\u001b[0m (49.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,959,402</span> (49.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m12,959,402\u001b[0m (49.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">87,303</span> (341.03 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m87,303\u001b[0m (341.03 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (300, 300, 3)\n",
    "\n",
    "optimizerAdam = AdamW(\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.004,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False,\n",
    "    clipnorm=None,\n",
    "    clipvalue=None,\n",
    "    global_clipnorm=None,\n",
    "    use_ema=False,\n",
    "    ema_momentum=0.99,\n",
    "    ema_overwrite_frequency=None,\n",
    "    loss_scale_factor=None,\n",
    "    gradient_accumulation_steps=None,\n",
    "    name='adamw',\n",
    ")\n",
    "\n",
    "base_model = EfficientNetB3(\n",
    "    include_top=False,            \n",
    "    weights='imagenet',        \n",
    "    input_shape=input_shape,     \n",
    "    pooling='avg'                \n",
    ")\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,                                 \n",
    "    layers.Dense(1024, activation='relu'),          \n",
    "    layers.Dense(512, activation='relu'),          \n",
    "    layers.Dense(256, activation='relu'),          \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.4),                         \n",
    "    layers.Dense(2, activation='softmax')           \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizerAdam,\n",
    "    loss='categorical_crossentropy',           \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'best_model.keras', monitor='val_loss', save_best_only=True, save_weights_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1902 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-1/bigGun\n",
      "Found 8094 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-1/handGun\n",
      "Found 1165 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-2/bigGun\n",
      "Found 197 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-2/handGun\n",
      "Found 1726 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-3/bigGun\n",
      "Found 7725 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-3/handGun\n",
      "Found 1699 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-4/bigGun\n",
      "Found 8313 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-4/handGun\n",
      "Found 1044 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-5/bigGun\n",
      "Found 356 images in /home/research/Kilsar_Sentinal/EfficientNet_data/dataset-5/handGun\n",
      "Total images collected: 32221\n",
      "Total labels collected: 32221\n",
      "Label distribution:\n",
      "handGun    24685\n",
      "bigGun      7536\n",
      "Name: count, dtype: int64\n",
      "Found 25777 validated image filenames belonging to 2 classes.\n",
      "Found 6444 validated image filenames belonging to 2 classes.\n",
      "Number of training images: 25777\n",
      "Number of validation images: 6444\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to collect image paths and labels from both datasets\n",
    "def collect_image_paths_and_labels(dataset_folders, class_names):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    # Iterate over dataset folders (dataset-1, dataset-2, etc.)\n",
    "    for dataset in dataset_folders:\n",
    "        for class_name in class_names:\n",
    "            class_folder = os.path.join(dataset, class_name)\n",
    "            class_image_paths = glob(os.path.join(class_folder, '*.jpg'))  # Adjust if images are not .jpg\n",
    "            \n",
    "            # Debug: Print the number of images found in each class\n",
    "            print(f'Found {len(class_image_paths)} images in {class_folder}')\n",
    "            \n",
    "            image_paths.extend(class_image_paths)\n",
    "            labels.extend([class_name] * len(class_image_paths))\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "# Define your datasets and class names\n",
    "dataset_folders = [\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet_data/dataset-1',\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet_data/dataset-2',\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet_data/dataset-3',\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet_data/dataset-4',\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet_data/dataset-5'\n",
    "]\n",
    "class_names = ['bigGun', 'handGun']  # Your class names\n",
    "\n",
    "# Collect image paths and labels\n",
    "image_paths, labels = collect_image_paths_and_labels(dataset_folders, class_names)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'filename': image_paths,\n",
    "    'class': labels\n",
    "})\n",
    "\n",
    "# Debug: Print total images collected\n",
    "print(f'Total images collected: {len(image_paths)}')\n",
    "print(f'Total labels collected: {len(labels)}')\n",
    "\n",
    "# Check the distribution of collected labels\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "print(\"Label distribution:\")\n",
    "print(label_counts)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "\n",
    "# Data augmentation and normalization\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2  # Split 20% for validation\n",
    ")\n",
    "\n",
    "# Create data generators for training and validation\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    df,\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Print out the number of images in each generator\n",
    "print(f\"Number of training images: {train_generator.n}\")\n",
    "print(f\"Number of validation images: {val_generator.n}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7996 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet data/splitted_data/train',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet data/splitted_data/val',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/research/Kilsar_Sentinal/.venv/lib64/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728520856.854215 1466927 service.cc:146] XLA service 0x7f648c002ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1728520856.854246 1466927 service.cc:154]   StreamExecutor device (0): NVIDIA A30, Compute Capability 8.0\n",
      "I0000 00:00:1728520856.854249 1466927 service.cc:154]   StreamExecutor device (1): NVIDIA A30, Compute Capability 8.0\n",
      "I0000 00:00:1728520856.854250 1466927 service.cc:154]   StreamExecutor device (2): NVIDIA A30, Compute Capability 8.0\n",
      "I0000 00:00:1728520856.854252 1466927 service.cc:154]   StreamExecutor device (3): NVIDIA A30, Compute Capability 8.0\n",
      "2024-10-09 19:40:57.793202: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-10-09 19:41:01.248394: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8902\n",
      "2024-10-09 19:41:05.771292: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38143', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2024-10-09 19:41:05.953851: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38143', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2024-10-09 19:41:06.004398: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38143', 168 bytes spill stores, 168 bytes spill loads\n",
      "\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1728520881.327703 1466927 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520881.483656 1466927 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520882.050292 1466927 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520882.207200 1466927 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2024-10-09 19:41:44.471022: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'loop_add_subtract_fusion_80', 8 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1728520904.774213 1466927 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 45/249\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 261ms/step - accuracy: 0.7489 - loss: 0.7458"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 19:42:04.299355: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38409', 36 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "2024-10-09 19:42:04.357272: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38143', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "2024-10-09 19:42:04.373895: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38407', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "2024-10-09 19:42:04.390944: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_38143', 196 bytes spill stores, 196 bytes spill loads\n",
      "\n",
      "E0000 00:00:1728520938.364898 1466930 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520938.520061 1466930 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520939.071397 1466930 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1728520939.228341 1466930 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 480ms/step - accuracy: 0.7890 - loss: 0.5739 - val_accuracy: 0.8085 - val_loss: 0.6623 - learning_rate: 0.0050\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/249\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 153ms/step - accuracy: 0.8438 - loss: 0.4583"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 19:43:44.331860: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2024-10-09 19:43:44.332479: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "/usr/lib64/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.8438 - loss: 0.4583 - val_accuracy: 0.9375 - val_loss: 0.6744 - learning_rate: 0.0050\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 19:43:51.546725: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.8102 - loss: 0.4390 - val_accuracy: 0.8100 - val_loss: 2.2421 - learning_rate: 0.0050\n",
      "Epoch 4/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166us/step - accuracy: 0.8438 - loss: 0.4391 - val_accuracy: 0.7500 - val_loss: 3.0821 - learning_rate: 1.0000e-03\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 19:44:59.963639: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 274ms/step - accuracy: 0.8213 - loss: 0.3973 - val_accuracy: 0.8100 - val_loss: 0.7882 - learning_rate: 1.0000e-03\n",
      "Epoch 6/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162us/step - accuracy: 0.8438 - loss: 0.2463 - val_accuracy: 0.7500 - val_loss: 1.0336 - learning_rate: 1.0000e-03\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint, lr_callback] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To unfreez layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 275ms/step - accuracy: 0.8637 - loss: 0.3334 - val_accuracy: 0.8105 - val_loss: 0.8046 - learning_rate: 1.0000e-03\n",
      "Epoch 2/5\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172us/step - accuracy: 0.8438 - loss: 0.3316 - val_accuracy: 0.7500 - val_loss: 0.8468 - learning_rate: 1.0000e-03\n",
      "Epoch 3/5\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 271ms/step - accuracy: 0.8652 - loss: 0.3239 - val_accuracy: 0.8105 - val_loss: 0.5964 - learning_rate: 1.0000e-03\n",
      "Epoch 4/5\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165us/step - accuracy: 0.9062 - loss: 0.2383 - val_accuracy: 0.6875 - val_loss: 0.8476 - learning_rate: 1.0000e-03\n",
      "Epoch 5/5\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.8823 - loss: 0.2917 - val_accuracy: 0.8100 - val_loss: 0.6549 - learning_rate: 1.0000e-03\n",
      "Epoch 1/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 394ms/step - accuracy: 0.8585 - loss: 0.3232 - val_accuracy: 0.8760 - val_loss: 0.3168 - learning_rate: 1.0000e-05\n",
      "Epoch 2/10\n",
      "\u001b[1m  1/249\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 156ms/step - accuracy: 0.9062 - loss: 0.2842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-09 20:37:38.624046: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 14ms/step - accuracy: 0.9062 - loss: 0.2842 - val_accuracy: 1.0000 - val_loss: 0.1659 - learning_rate: 1.0000e-05\n",
      "Epoch 3/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 274ms/step - accuracy: 0.8660 - loss: 0.3110 - val_accuracy: 0.8458 - val_loss: 0.3673 - learning_rate: 1.0000e-05\n",
      "Epoch 4/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165us/step - accuracy: 0.9062 - loss: 0.2650 - val_accuracy: 0.8750 - val_loss: 0.4229 - learning_rate: 1.0000e-05\n",
      "Epoch 5/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 272ms/step - accuracy: 0.8789 - loss: 0.2945 - val_accuracy: 0.8140 - val_loss: 0.5905 - learning_rate: 1.0000e-05\n",
      "Epoch 6/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166us/step - accuracy: 0.8750 - loss: 0.2435 - val_accuracy: 0.7500 - val_loss: 0.8426 - learning_rate: 2.0000e-06\n",
      "Epoch 7/10\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 273ms/step - accuracy: 0.8756 - loss: 0.3009 - val_accuracy: 0.8967 - val_loss: 0.2659 - learning_rate: 2.0000e-06\n"
     ]
    }
   ],
   "source": [
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Initial training with frozen base model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint]\n",
    ")\n",
    "\n",
    "# Unfreeze base model and fine-tune with a lower learning rate\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=AdamW(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size,\n",
    "    callbacks=[reduce_lr, early_stopping, checkpoint]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    # Build a model with hyperparameters to tune\n",
    "    base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_shape, pooling='avg')\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Dense(hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'),\n",
    "        layers.Dense(hp.Int('units2', min_value=32, max_value=256, step=32), activation='relu'),\n",
    "        layers.Dropout(hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [1e-3, 1e-4, 1e-5])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Create a tuner and search for the best hyperparameters\n",
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    directory='keras_tuner',\n",
    "    project_name='efficientnet_tuning'\n",
    ")\n",
    "tuner.search(train_generator, validation_data=val_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "\n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "base_model = EfficientNetB3(\n",
    "    include_top=False,            \n",
    "    weights='imagenet',        \n",
    "    input_shape=input_shape,     \n",
    "    pooling='avg'                \n",
    ")\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,                                 \n",
    "    layers.Dense(256, activation='relu'),          \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),                         \n",
    "    layers.Dense(2, activation='softmax')           \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',           \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary of the custom model\n",
    "model.summary()\n",
    "\n",
    "# Training\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet data/splitted_data/train',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    '/home/research/Kilsar_Sentinal/EfficientNet data/splitted_data/val',\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_steps=val_generator.samples // val_generator.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/research/Kilsar_Sentinal/suryansh/src/models/tf models/model-2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('efficientnet_b0_saved_model')\n",
    "# Optional: apply optimizations like quantization\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Convert the model\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted .tflite model\n",
    "with open('efficientnet_b0.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the EfficientNetB3 model\n",
    "model = tf.keras.applications.EfficientNetB3(\n",
    "    include_top=True,\n",
    "    weights='imagenet',  \n",
    "    input_shape=(300, 300, 3) \n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "# Define input shape based on the dataset (e.g., 300x300 RGB images)\n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "# Load the EfficientNet-B3 model with custom settings\n",
    "base_model = EfficientNetB3(\n",
    "    include_top=False,            # Remove the default top layer to add custom layers\n",
    "    weights='imagenet',           # Use pre-trained ImageNet weights\n",
    "    input_shape=input_shape,      # Set input shape\n",
    "    pooling='avg'                 # Apply global average pooling\n",
    ")\n",
    "\n",
    "# Freeze the base model layers to retain pre-trained features\n",
    "base_model.trainable = False\n",
    "\n",
    "# Create a new model on top of the base model\n",
    "model = models.Sequential([\n",
    "    base_model,                                    # EfficientNetB3 base model\n",
    "    layers.Dense(256, activation='relu'),          # Custom fully connected layer for feature extraction\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.2),                           # Dropout layer for regularization\n",
    "    layers.Dense(2, activation='softmax')          # Output layer with 2 classes (for 2 gun types)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',               # Use categorical cross-entropy for multi-class classification\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary of the custom model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 27m 48s]\n",
      "val_accuracy: 0.8097798824310303\n",
      "\n",
      "Best val_accuracy So Far: 0.8405869603157043\n",
      "Total elapsed time: 02h 55m 11s\n",
      "Best Hyperparameters: {'dense_units_1': 512, 'dense_units_2': 256, 'dropout_rate': 0.4, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "train_dir = '/home/research/Kilsar_Sentinal/EfficientNet data/splitted_data/train'\n",
    "\n",
    "# enabling gpu\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth set successfully.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "\n",
    "# Define a function for model building that integrates with Keras Tuner\n",
    "def build_model(hp):\n",
    "    base_model = EfficientNetB3(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=(300, 300, 3),\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Create the model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Dense(hp.Int('dense_units_1', min_value=128, max_value=512, step=128), activation='relu'),\n",
    "        layers.Dense(hp.Int('dense_units_2', min_value=64, max_value=256, step=64), activation='relu'),\n",
    "        layers.Dropout(hp.Float('dropout_rate', min_value=0.2, max_value=0.4, step=0.1)),\n",
    "        layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create a tuner instance\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='tuner_dir',\n",
    "    project_name='hyperparam_tuning'\n",
    ")\n",
    "\n",
    "# Prepare training data\n",
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Perform the search\n",
    "tuner.search(train_generator, epochs=10, validation_data=train_generator)\n",
    "\n",
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters: {best_hps.values}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
