{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 16:38:18.258103: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-16 16:38:18.269322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-16 16:38:18.280827: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-16 16:38:18.284279: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-16 16:38:18.294547: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-16 16:38:18.828319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 16:39:06.822390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22400 MB memory:  -> device: 0, name: NVIDIA A30, pci bus id: 0000:4a:00.0, compute capability: 8.0\n",
      "2024-10-16 16:39:06.824441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22400 MB memory:  -> device: 1, name: NVIDIA A30, pci bus id: 0000:61:00.0, compute capability: 8.0\n",
      "2024-10-16 16:39:06.826394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22400 MB memory:  -> device: 2, name: NVIDIA A30, pci bus id: 0000:ca:00.0, compute capability: 8.0\n",
      "2024-10-16 16:39:06.828605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 22400 MB memory:  -> device: 3, name: NVIDIA A30, pci bus id: 0000:e1:00.0, compute capability: 8.0\n",
      "/home/research/Kilsar_Sentinal/.venv/lib64/python3.11/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from tensorflow import keras\n",
    "\n",
    "# Load YOLOv9c model (PyTorch)\n",
    "yolo_model = YOLO('/home/research/Kilsar_Sentinal/Mohammad/runs/detect/train47/weights/best.pt')\n",
    "# yolo_model = torch.hub.load('ultralytics/yolov9c', 'custom', path='/home/research/Kilsar_Sentinal/Mohammad/runs/detect/train47/weights/best.pt')\n",
    "\n",
    "# Load EfficientNet model (Keras)\n",
    "effnet_model = keras.models.load_model('/home/research/Kilsar_Sentinal/suryansh/src/models/tf models/model-1.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv9c summary: 618 layers, 25,530,003 parameters, 0 gradients, 103.7 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">393,472</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ efficientnetb3 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │    \u001b[38;5;34m10,783,535\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m393,472\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,636,789</span> (44.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,636,789\u001b[0m (44.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,626</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m426,626\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,783,535</span> (41.14 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m10,783,535\u001b[0m (41.14 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">426,628</span> (1.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m426,628\u001b[0m (1.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yolo_model.info()\n",
    "effnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import os\n",
    "\n",
    "# Preprocess image for EfficientNet\n",
    "def preprocess_for_effnet(crop):\n",
    "    crop_resized = cv2.resize(crop, (300, 300))  # Resize to 300x300 for EfficientNet B3\n",
    "    crop_preprocessed = preprocess_input(crop_resized)  # Apply EfficientNet preprocessing\n",
    "    crop_preprocessed = np.expand_dims(crop_preprocessed, axis=0)  # Add batch dimension\n",
    "    return crop_preprocessed\n",
    "\n",
    "# Detection and classification pipeline for a single image\n",
    "def detect_and_classify(image_path, output_folder):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect guns using YOLOv9c\n",
    "    results = yolo_model(image)\n",
    "    print(results[0].boxes)\n",
    "    # gun_boxes = results.xyxy[0]  # Extract bounding boxes for detected guns\n",
    "    gun_boxes = results.pred[0].cpu().numpy()\n",
    "\n",
    "    # For each detected gun, classify it using EfficientNet\n",
    "    for box in gun_boxes:\n",
    "        x1, y1, x2, y2, conf, cls = map(int, box[:6])\n",
    "        gun_crop = image[y1:y2, x1:x2]  # Crop the gun region\n",
    "\n",
    "        # Preprocess for EfficientNet\n",
    "        gun_crop_preprocessed = preprocess_for_effnet(gun_crop)\n",
    "\n",
    "        # Classify the gun with EfficientNet\n",
    "        predictions = effnet_model.predict(gun_crop_preprocessed)\n",
    "        predicted_class = np.argmax(predictions, axis=1)\n",
    "        class_label = 'Class 1' if predicted_class == 0 else 'Class 2'\n",
    "\n",
    "        # Annotate the image with classification result\n",
    "        cv2.putText(image, f'{class_label}', (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Save the result to the specified output folder\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f\"output_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_filename, image)\n",
    "\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import os\n",
    "\n",
    "# Preprocess image for EfficientNet\n",
    "def preprocess_for_effnet(crop):\n",
    "    crop_resized = cv2.resize(crop, (300, 300))  # Resize to 300x300 for EfficientNet B3\n",
    "    crop_preprocessed = preprocess_input(crop_resized)  # Apply EfficientNet preprocessing\n",
    "    crop_preprocessed = np.expand_dims(crop_preprocessed, axis=0)  # Add batch dimension\n",
    "    return crop_preprocessed\n",
    "\n",
    "# Detection and classification pipeline for a single image\n",
    "def detect_and_classify(image_path, output_folder):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect guns using YOLOv9c\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Extract bounding boxes for detected guns\n",
    "    gun_boxes = results[0].boxes\n",
    "\n",
    "    # Initialize counts for each class\n",
    "    class_counts = {'Class 1': 0, 'Class 2': 0}\n",
    "\n",
    "    # For each detected gun, classify it using EfficientNet\n",
    "    if gun_boxes is not None and len(gun_boxes) > 0:\n",
    "        for box in gun_boxes:\n",
    "            # Get bounding box coordinates, confidence, and class\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "            conf = box.conf[0].item()\n",
    "            cls = int(box.cls[0].item())\n",
    "\n",
    "            # Use names dictionary for class label (if applicable)\n",
    "            class_label = results[0].names[cls]\n",
    "\n",
    "            # Crop the gun region from the image\n",
    "            gun_crop = image[y1:y2, x1:x2]\n",
    "\n",
    "            # Preprocess for EfficientNet\n",
    "            gun_crop_preprocessed = preprocess_for_effnet(gun_crop)\n",
    "\n",
    "            # Classify the gun with EfficientNet\n",
    "            predictions = effnet_model.predict(gun_crop_preprocessed)\n",
    "            predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "            # Update counts based on the predicted class\n",
    "            if predicted_class == 0:\n",
    "                class_counts['Class 1'] += 1\n",
    "            else:\n",
    "                class_counts['Class 2'] += 1\n",
    "\n",
    "            # Annotate the image with classification result\n",
    "            cv2.putText(image, f'{class_label} ({conf:.2f})', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Save the result to the specified output folder\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f\"output_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_filename, image)\n",
    "\n",
    "    # Print the counts of each class\n",
    "    # print(f\"Total Class 1: {class_counts['Class 1']}\")\n",
    "    # print(f\"Total Class 2: {class_counts['Class 2']}\")\n",
    "\n",
    "    return output_filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working code correct with results iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess image for EfficientNet\n",
    "def preprocess_for_effnet(crop):\n",
    "    crop_resized = cv2.resize(crop, (300, 300))  # Resize to 300x300 for EfficientNet B3\n",
    "    crop_preprocessed = preprocess_input(crop_resized)  # Apply EfficientNet preprocessing\n",
    "    crop_preprocessed = np.expand_dims(crop_preprocessed, axis=0)  # Add batch dimension\n",
    "    return crop_preprocessed\n",
    "\n",
    "\n",
    "# Detection and classification pipeline for a single image\n",
    "def detect_and_classify(image_path, output_folder):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Detect guns using YOLOv9c\n",
    "    results = yolo_model(image)\n",
    "\n",
    "    # Initialize counts for each class\n",
    "    class_counts = {'Class 1': 0, 'Class 2': 0}\n",
    "    class_predictions = []  # List to store predictions for this image\n",
    "\n",
    "    # Iterate over the list of results\n",
    "    for result in results:\n",
    "        # Extract bounding boxes for detected guns\n",
    "        gun_boxes = result.boxes\n",
    "\n",
    "        # For each detected gun, classify it using EfficientNet\n",
    "        if gun_boxes is not None and len(gun_boxes) > 0:\n",
    "            for box in gun_boxes:\n",
    "                # Get bounding box coordinates, confidence, and class\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                conf = box.conf[0].item()\n",
    "                cls = int(box.cls[0].item())\n",
    "\n",
    "                # Use names dictionary for class label (if applicable)\n",
    "                class_label = result.names[cls]\n",
    "\n",
    "                # Crop the gun region from the image\n",
    "                gun_crop = image[y1:y2, x1:x2]\n",
    "\n",
    "                # Preprocess for EfficientNet\n",
    "                gun_crop_preprocessed = preprocess_for_effnet(gun_crop)\n",
    "\n",
    "                # Classify the gun with EfficientNet\n",
    "                predictions = effnet_model.predict(gun_crop_preprocessed)\n",
    "                predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "                # Check confidence for the predicted class\n",
    "                class_confidence = predictions[0][predicted_class]  # Get confidence of the predicted class\n",
    "\n",
    "                # Update counts based on the predicted class\n",
    "                if class_confidence < 0.5:\n",
    "                    # Print statement if confidence is below threshold\n",
    "                    print(f\"Low confidence for {class_label} : {class_confidence[0]:.2f}. Skipping bounding box.\")\n",
    "                    continue  # Skip this box and do not draw the bounding box\n",
    "\n",
    "                if predicted_class == 0:\n",
    "                    class_counts['Class 1'] += 1\n",
    "                    class_predictions.append('Class 1')\n",
    "                else:\n",
    "                    class_counts['Class 2'] += 1\n",
    "                    class_predictions.append('Class 2')\n",
    "\n",
    "                # Annotate the image with classification result\n",
    "                cv2.putText(image, f'{class_predictions[-1]} ({conf:.2f})', (x1, y1 - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    # Save the result to the specified output folder\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    output_filename = os.path.join(output_folder, f\"output_{os.path.basename(image_path)}\")\n",
    "    cv2.imwrite(output_filename, image)\n",
    "\n",
    "    # Print the counts of each class\n",
    "    print(f\"Total Class 1: {class_counts['Class 1']}\")\n",
    "    print(f\"Total Class 2: {class_counts['Class 2']}\")\n",
    "\n",
    "    # Log results for this image\n",
    "    print(f\"Predictions for {os.path.basename(image_path)}: {class_predictions}\")\n",
    "\n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 448x640 (no detections), 11.6ms\n",
      "Speed: 3.7ms preprocess, 11.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Total Class 1: 0\n",
      "Total Class 2: 0\n",
      "Predictions for istockphoto-1041100896-612x612.jpg: []\n",
      "Processed /home/research/Kilsar_Sentinal/Data_results/demo/istockphoto-1041100896-612x612.jpg, saved result to /home/research/Kilsar_Sentinal/Data_results/cycle-6/output_istockphoto-1041100896-612x612.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Folder paths\n",
    "input_folder = '/home/research/Kilsar_Sentinal/Data_results/demo'\n",
    "output_folder = '/home/research/Kilsar_Sentinal/Data_results/cycle-6'\n",
    "\n",
    "# Process all images in the input folder\n",
    "for image_file in os.listdir(input_folder):\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "    if image_file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "        output_image_path = detect_and_classify(image_path, output_folder)\n",
    "        print(f\"Processed {image_path}, saved result to {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "\n",
    "# cap = cv2.VideoCapture(0)  # Capture from webcam\n",
    "\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # Process the frame for detection and classification\n",
    "#     ...\n",
    "    \n",
    "#     # Display the frame\n",
    "#     cv2.imshow('Gun Detection and Classification', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
